# PyTroch

<!-- ì´ê±° ì“°ì‚¼
<ul>
  <li><h3>ë°ì´í„° ë¡œë” ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê·¸ë¦¬ë“œ ì¶œë ¥</h3></li>
</ul>

```python

```
-->

## ëª¨ë¸ ìƒì„± ë° í•™ìŠµ íë¦„

<ul>
  <li><h3>ëª¨ë¸ ì§ì ‘ êµ¬í˜„</h3></li>
</ul>

```python
# CNNê³¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì¤‘ ì–´ë–¤ ê²ƒì„ ì“¸ì§€ í˜¼í•©í•  ì§€ ê³ ë¯¼í•´ë¼
# cnnì€ ì–¼êµ´ê°™ì€ ê°€ê¹Œìš´ ê±° ì˜ì¡ê³ , íŠ¸ëœì€ ë©€ê³  ë°ì´í„°ê°€ ë§ì•„ì•¼ë§Œ ì˜ ì¡ìŒ
class MyModel(nn.Module): 
  def __init__(self):
    super().__init__()

    self.layer1 = nn.Sequential(
        # ì…ë ¥ ì´ë¯¸ì§€ ì°¨ì›(í‘ë°± : 1/ ì»¬ëŸ¼ : 3)/ ì»¤ë„ ê°œìˆ˜(convì™€ batchê°€ ê°œìˆ˜ê°€ ê°™ìŒ)
        nn.Conv2d(3, 16, kernel_size = 3, stride = 2, padding = 0),
        nn.BatchNorm2d(16),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2, stride = 2)
    )

    self.layer2 = nn.Sequential(
        nn.Conv2d(16, 32, kernel_size = 3, stride = 2, padding = 0),
        nn.BatchNorm2d(32),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2, stride = 2)
    )

    self.layer3 = nn.Sequential(
        nn.Conv2d(32, 64, kernel_size = 3, stride = 2, padding = 0),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2, stride = 2)
    )

    self.drop_out = nn.Dropout(0.5)

    self.fc1 = nn.Linear(3 * 3 * 64, 1000, bias = True)
    self.fc2 = nn.Linear(1000, 1)

  def forward(self, x):
    out = self.layer1(x)
    out = self.layer2(out)
    out = self.layer3(out)

    out = out.view(out.size(0), -1)
    out = self.drop_out(out)
    out = self.fc1(out)
    out = self.fc2(out)
    return out
```

<ul>
  <li><h3>ëª¨ë¸ ì§ì ‘ êµ¬í˜„ list, forë¬¸ í™œìš©</h3></li>
</ul>

```python
class ConvolutionalNeuralNetworkClass(nn.Module):
    """
        Convolutional Neural Network (CNN) Class
    """
    def __init__(self,name='cnn',xdim=[1,28,28],
                 ksize=3,cdims=[32,64],hdims=[1024,128],ydim=10,
                 USE_BATCHNORM=False):
        super(ConvolutionalNeuralNetworkClass,self).__init__()
        self.name = name
        self.xdim = xdim
        self.ksize = ksize
        self.cdims = cdims
        self.hdims = hdims
        self.ydim = ydim
        self.USE_BATCHNORM = USE_BATCHNORM

        # Convolutional layers
        self.layers = [] # ëª¨ë“  ë ˆì´ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ìœ¼ê³  ë‚˜ì¤‘ì— Sequentialì— ë„£ì„ ê²ƒì´ë‹¤.
        prev_cdim = self.xdim[0] # ìµœì´ˆ ì…ë ¥ ì´ë¯¸ì§€ ì°¨ì› ì´ˆê¸°í™”
        for cdim in self.cdims: # cnn ì¶œë ¥ ì±„ë„ ìˆ˜ [32,64]ë§Œí¼ forë¬¸ ìˆ˜í–‰
            self.layers.append(
                nn.Conv2d(
                    # FILL IN HERE
                    in_channels = prev_cdim,
                    out_channels = cdim,
                    kernel_size = self.ksize,
                    stride = (1,1),
                    padding = self.ksize//2
                ) # convlution
            )

            if self.USE_BATCHNORM:
                self.layers.append(nn.BatchNorm2d(cdim)) # batch-norm

            self.layers.append(nn.ReLU(True))  # activation # conv ë ˆì´ì–´ ë°‘ì— í•˜ë‚˜í•˜ë‚˜ ë ë£¨ ë“±ì„ ë¶™ì„
            self.layers.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))) # max-pooling
            self.layers.append(nn.Dropout2d(p=0.5))  # dropout
            prev_cdim = cdim # forë¬¸ í•œë²ˆ ëë‚˜ë©´ ìƒˆë¡œìš´ convë¥¼ ë§Œë“¤í…ë° ìƒˆë¡œìš´ convì˜ ì…ë ¥ì„ ì´ì „ convì˜ ì¶œë ¥ ì°¨ì›ê³¼ ë§ì¶¤

        # Dense layers
        self.layers.append(nn.Flatten())
        prev_hdim = prev_cdim*(self.xdim[1]//(2**len(self.cdims)))*(self.xdim[2]//(2**len(self.cdims))) # convì˜ ì¶œë ¥ê³¼ densì˜ ì…ë ¥ì„ ë§ì¶¤
        for hdim in self.hdims:
            self.layers.append(nn.Linear(
                # FILL IN HERE
                in_features = prev_hdim,
                out_features = hdim,
                bias = True
                ))
            self.layers.append(nn.ReLU(True))  # activation
            prev_hdim = hdim
        # Final layer (without activation)
        self.layers.append(nn.Linear(prev_hdim,self.ydim,bias=True))


        # Concatenate all layers #
        self.net = nn.Sequential()
        for l_idx,layer in enumerate(self.layers):
            layer_name = "%s_%02d"%(type(layer).__name__.lower(),l_idx)
            self.net.add_module(layer_name,layer)
        self.init_param() # initialize parameters

    def init_param(self):
        for m in self.modules():
            if isinstance(m,nn.Conv2d): # init conv
                nn.init.kaiming_normal_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m,nn.BatchNorm2d): # init BN
                nn.init.constant_(m.weight,1)
                nn.init.constant_(m.bias,0)
            elif isinstance(m,nn.Linear): # lnit dense
                nn.init.kaiming_normal_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self,x):
        return self.net(x)

C = ConvolutionalNeuralNetworkClass(
    name='cnn',xdim=[1,28,28],ksize=3,cdims=[32,64],
    hdims=[32],ydim=10).to(device)
```

<ul>
  <li><h3>í•™ìŠµ í”„ë¡œì„¸ìŠ¤</h3></li>
</ul>

```python
# ğŸš¨ í•™ìŠµ ì†ë„ ìµœì í™” ë°©ë²•(GradScaler, zero_grad non_blocking) ë“¤ì–´ ìˆìŒ
# ê°„ë‹¨í•œ ê±´ ml basic ë”°ë¼ì¹˜ê¸° 1
train_dataset = (ë°ì´í„° ë¡œë” í™œìš© ë¸”ë¡œê·¸, ë ˆë²¨ 1 ì»¤ìŠ¤í…€ ë°ì´í„° ì…‹ ì°¸ê³ )
test_dataset = (")

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# í•™ìŠµ ì„¤ì •ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.
EPOCHS = 100
BATCH_SIZE = 64
LEARNING_RATE = 0.1

model.to(device)
criti = nn.BCEWithLogitsLoss()
# ì˜µí‹°ë§ˆì´ì €ë¥¼ ì–´ë–¤ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ í•  ê²ƒì¸ì§€
opt = optim.Adam(model.parameters(), lr = LEARNING_RATE)
scaler = torch.cuda.amp.GradScaler()

# ê°„ë‹¨í•˜ê²Œ : ì•„ë˜ ì½”ë“œ, ML basic ë”°ë¼ì¹˜ê¸° 2 ì°¸ê³ 
# ê¹Šê²Œ[ì•½ê°„ ì¶”ìƒí™”] : íŒŒíƒ¬ ì°¸ê³ / ê¹Šê²Œ[ì¶”ìƒí™”] : ë ˆë²¨ 2ì°¸ê³ 
# ë¡œìŠ¤ : ë ˆë²¨ 1 ì°¸ê³ 
for epoch in range(1, EPOCHS+1):
  epoch_loss = 0
  epoch_acc = 0
  # ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì„ train ëª¨ë“œë¡œ ë‘ì–´ gradientì„ ê³„ì‚°í•˜ê³ , ì—¬ëŸ¬ sub module (ë°°ì¹˜ ì •ê·œí™”, ë“œë¡­ì•„ì›ƒ ë“±)ì´ train modeë¡œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ í•¨
  model.train() 

  for X_batch, y_batch in dataloader:
    X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True).type(torch.cuda.FloatTensor)
  
    opt.zero_grad(set_to_none = True)
    with torch.cuda.amp.autocast():
      y_pred = model(X_batch)
      y_pred = torch.argmax(y_pred, dim=-1)
      loss = criti(y_pred, y_batch.unsqueeze(1)) # yë¥¼ 1í–‰ nì—´ì´ ì•„ë‹Œ ní–‰ 1ì—´ë¡œ ë§Œë“¬ (n, )ê°€ (n, 1)ë¡œ ë¨

    acc = binary_acc(y_pred, y_batch.unsqueeze(1))
    epoch_loss += loss.item() # tensor([3]) í…ì„œì—ì„œ ê°’(3)ë§Œ ê°€ì ¸ì˜¤ê¸°
    epoch_acc += acc.item()

    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': opt.state_dict(),
        'loss': epoch_loss,
        }, f"{PATH}/checkpoint_model_{epoch}_{epoch_loss/len(dataloader)}_{epoch_acc/len(dataloader)}.pt")

    # ì—í­ë³„ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(dataloader):.5f} | Acc: {epoch_acc/len(dataloader):.3f}')
```

<ul>
  <li><h3>ì¸í¼ëŸ°ìŠ¤</h3></li>
</ul>

```python
# íŒŒíƒ¬ ì°¸ê³ / ì¶”ìƒí™” : ë ˆë²¨ 2ì°¸ê³ 
data_loader = í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ë¡œë“œ
model = ëª¨ë¸ êµ¬ê²© ë¡œë“œ
criterion = ë¡œìŠ¤ ë¡œë“œ
metrics = ì§€í‘œ ë¡œë“œ
checkpoint = torch.load()
state_dict = checkpoint['state_dict']
model.load_state_dict(state_dict) # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ì‚½ì…

# ì¸í¼ëŸ°ìŠ¤(ë¡œìŠ¤ logì— ì¶œë ¥)
with torch.no_grad():
    for i, (data, target) in enumerate(tqdm(data_loader)):
        data, target = data.to(device), target.to(device)
        output = model(data)
        
        loss = criterion(output, target)
```

<ul>
  <li><h3>êµì°¨ ê²€ì¦ í•™ìŠµ</h3></li>
</ul>

```python
# íŒŒíƒ¬ ì°¸ê³ 
def kFoldTrain(config):
    # 5-fold Stratified KFold 5ê°œì˜ foldë¥¼ í˜•ì„±í•˜ê³  5ë²ˆ Cross Validationì„ ì§„í–‰í•©ë‹ˆë‹¤.
    n_splits = 2

    skf = StratifiedKFold(n_splits=n_splits)
    patience = 10
    dataset = config.init_obj("dataset", module_dataset)
    labels = dataset.y

    # Stratified KFoldë¥¼ ì‚¬ìš©í•´ Train, Valid foldì˜ Indexë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    # labels ë³€ìˆ˜ì— ë‹´ê¸´ í´ë˜ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Stratifyë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
    # ë§¤ ì´í„°ë ˆì´ì…˜ ì´ kê°œ
    for i, (train_idx, valid_idx) in enumerate(skf.split(dataset.image_list, labels)):
```

<ul>
  <li><h3>ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ</h3></li>
</ul>


```python
PATH = "saved"

if(not os.path.exists(PATH)):
  os.makedirs(PATH)

# 1. ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
torch.save(model.state_dict(), os.path.join(PATH, "model.pt"))

# ë¡œë“œ
new_model = MyModel()
new_model.load_state_dict(torch.load(os.path.join(PATH, "model.pt")))

# 2. ì „ì²´ ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
torch.save(model, os.path.join(PATH, "model_pickle.pt"))

# ì €ì¥ëœ ì „ì²´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
model = torch.load(os.path.join(PATH, "model_pickle.pt"))

# 3. ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ
checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint["model_state_dict"])
opt.load_state_dict(checkpoint["optimizer_state_dict"])
epoch = checkpoint("epoch")

# í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì€ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ë‘ì–´ ì—¬ëŸ¬ sub moduleë“¤ì´ eval modeë¡œ ì‘ë™í•  ìˆ˜ ìˆê²Œ í•¨
model.eval()
```

<ul>
  <li><h3>ì „ì´ í•™ìŠµ êµ¬í˜„</h3></li>
</ul>


```python
# íŒŒì´í† ì¹˜ ì‹¬í™” ê³¼ì œ 1 ì°¸ê³ 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg = models.vgg16(pretrained = True).to(device)

# ëª¨ë¸ì˜ ëª¨ë“ˆ ë°”ê¿€ ìˆ˜ ìˆìŒ(ì•ˆ ë°”ê¾¸ëŠ” ê²Œ ì¢‹ìŒ)
for name, layer in vgg.named_modules():
  print(name, layer)

# vgg.fc = torch.nn.Linear(1000, 1)
# vgg.classifier._modules['6'] = torch.nn.Linear(4096, 1)

import torch.nn as nn

class MyVgg(nn.Module):
  def __init__(self):
    super().__init__()
    self.vgg19 = models.vgg19(pretrained = True)
    self.linear_layers = nn.Linear(1000, 1, bias = True)
    nn.init.xavier_uniform_(self.linear_layers.weight)
    # biasë¥¼ ì´ˆê¸°í™”
    stdv = 1. / math.sqrt(self.linear_layers.weight.size(1))
    self.linear_layers.bias.data.uniform_(-stdv, stdv)

  def forward(self, x):
    x = self.vgg19(x)
    return self.linear_layers(x)

myVgg = MyVgg()

# freeze ì „ì´ í•™ìŠµ
for param in myVgg.parameters():
  param.requires_grad = False

for param in myVgg.linear_layers.parameters()  :
  param.requires_grad = True

dataloader = torch.utils.data.DataLoader(dataset,
                                         batch_size = BATCH_SIZE,
                                         shuffle = True,
                                         num_workers = 8)

for epoch in range(1, EPOCHS+1):
  # ~~
```

## ìœ ìš©í•œ í•¨ìˆ˜
<ul>
  <li><h3>ê°€ì¤‘ì¹˜ ìƒ˜í”ŒëŸ¬</h3></li>
</ul>

```python
# ë°ì´í„° ì…‹ í´ë˜ìŠ¤ ë„¤ë¶€ì— yë¥¼ Counter ëŒë ¤ì„œ ë¶„í¬ í™•ì¸í•˜ëŠ” ë©”ì„œë“œ
def target_class_distribution(self):
    target_dist = Counter(self.y)
    return target_dist, self.y

# ì „ì²´ ë°ì´í„° ì…‹
def getSampler(train_dataset):
    target_dist, y_train = train_dataset.target_class_distribution()
    sort_target_dist = sorted(target_dist.items(), key = lambda x:x[0])
    num_samples = len(train_dataset)
    class_weights = [num_samples / sort_target_dist[i][1] for i in range(len(sort_target_dist))]
    weights = [class_weights[y_train[i]] for i in range(num_samples)] # í•´ë‹¹ ë ˆì´ë¸”ë§ˆë‹¤ì˜ ê°€ì¤‘ì¹˜ ë¹„ìœ¨
    return WeightedRandomSampler(torch.DoubleTensor(weights), int(15120))

# spilt í•œ ê²½ìš°
train_set, val_set = ë°ì´í„° ì…‹ ë¶„ë¦¬ ìƒíƒœ
_, multi_class_labels = train_dataset.target_class_distribution()
multi_class_labels = np.array(multi_class_labels)
# ì´ê±´ ë¶„ë¦¬í•œ ì…‹ì´ Subset íƒ€ì…ì¼ ê²½ìš° ì›ë³¸ì—ì„œì˜ ì¸ë±ìŠ¤ ë°˜í™˜ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ˆì‹œ
train_set_y = multi_class_labels[train_set.indices]
target_dist = Counter(train_set_y)

def getSampler(target_dist, train_set_y):
    sort_target_dist = sorted(target_dist.items(), key = lambda x:x[0])
    num_samples = len(train_set_y)
    class_weights = [num_samples / sort_target_dist[i][1] for i in range(len(sort_target_dist))]
    
    weights = [class_weights[train_set_y[i]] for i in range(int(num_samples))] #í•´ë‹¹ ë ˆì´ë¸”ë§ˆë‹¤ì˜ ê°€ì¤‘ì¹˜ ë¹„ìœ¨
    return WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))
```

<ul>
  <li><h3>í´ë˜ìŠ¤ ë³„ Grayscale ê·¸ë˜í”„</h3></li>
</ul>

```python
# ê¸°ì´ˆ í”Œì  ë”°ë¼ì¹˜ê¸° EDA 
plt.figure()
plt.subplot(111)

for class_id in num2class:
    img = np.array(Image.open(os.path.join(cfg.img_dir, img_id, class_id+ext)).convert('L'))
    histogram, bin_edges = np.histogram(img, bins=256, range=(0, 255))
    sns.lineplot(data=histogram)

plt.legend(num2class)
plt.title('Class Grayscale Histogram Plot', fontsize=15)
plt.show()
```

<ul>
  <li><h3>ë°ì´í„° ë¡œë” ë°°ì¹˜ í¬ê¸°ë§Œí¼ ê·¸ë¦¬ë“œ ì¶œë ¥</h3></li>
</ul>

```python
# ë¬´ì‘ìœ„ í•™ìŠµ ì´ë¯¸ì§€ ëª‡ ê°œ ì„ íƒ
dataiter = iter(trainloader)
images, labels = next(iter(dataiter))

# ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ìƒì„±
img_grid = torchvision.utils.make_grid(images)

# ì´ë¯¸ì§€ ì¶œë ¥
matplotlib_imshow(img_grid, one_channel=True)
```

<ul>
  <li><h3>ì´ë¯¸ì§€ê°€ í‘ë°± 1ì¼ ë•Œ ëª¨ë¸ì´ 3ì´ë©´ </h3></li>
</ul>

```python
# ë°ì´í„°ë¥¼ 3ìœ¼ë¡œ ë³€í™˜
common_transform = torchvision.transforms.Compose(
  [
    torchvision.transforms.Grayscale(num_output_channels=3), # grayscaleì˜ 1ì±„ë„ ì˜ìƒì„ 3ì±„ë„ë¡œ ë™ì¼í•œ ê°’ìœ¼ë¡œ í™•ì¥í•¨
    torchvision.transforms.ToTensor() # PIL Imageë¥¼ Tensor typeë¡œ ë³€ê²½í•¨
  ]
)
```

```python
# ëª¨ë¸ì„ 1ë¡œ ë³€í™˜
target_model.conv1 = torch.nn.Conv2d(FASHION_INPUT_NUM, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
```

<ul>
  <li><h3>í† ì¹˜ ì„œë¨¸ë¦¬</h3></li>
</ul>

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ì‚¬ì „ í•™ìŠµëœ VGG16 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
imagenet_resnet18 = torchvision.models.resnet18(pretrained=True).to(device)
# ëª¨ë¸ì˜ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ìš”ì•½ ì¶œë ¥
summary(imagenet_resnet18, (3, 224, 224))
```

<ul>
  <li><h3>íŒŒì´ì¬ Enum</h3></li>
</ul>

```python
class Age(int, Enum):
    """Enum í´ë˜ìŠ¤ -> enumë§Œ ëª¨ì•„ë‘ì"""
    
    YOUNG = 0
    MIDDLE = 1
    OLD = 2
    
    @classmethod
    def _label_ages(cls, ages):
        if(ages < 30):
            return cls.YOUNG
        elif(ages < 60):
            return cls.MIDDLE
        else:
            return cls.OLD
```

<ul>
  <li><h3>ë°ì´í„° ì…‹ ë¼ë²¨ ë”ë¯¸í™”</h3></li>
</ul>

```python
# dicë¡œ ë”ë¯¸í™”
def _convert_class_to_numeric(self, class_array):
    class_to_number = {class_value : idx for idx, class_value in enumerate(np.unique(class_array))}
    return np.vectorize(class_to_number.get)(class_array)

# ifë¬¸ìœ¼ë¡œ ë”ë¯¸í™”
def _convert_age_to_dummy(self, ages):
    return np.vectorize(Age._label_ages)(ages)
```

<ul>
  <li><h3>json kfold split</h3></li>
</ul>

```python
annotation = "./cleansing.json"
output_dir = './json_folder'

with open(annotation) as f:
    data = json.load(f)
var = [(ann['image_id'], ann['category_id']) for ann in data['annotations']]
X = np.ones((len(data['annotations']), 1))
y = np.array([v[1] for v in var])
groups = np.array([v[0] for v in var])
cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=207)

for i,(train_idx, val_idx) in enumerate(cv.split(X, y, groups), start = 1) :
    train_data_img_ids = set([data['annotations'][idx]['image_id'] for idx in train_idx])
    train_data_imgs = [img for img in data['images'] for img_id in train_data_img_ids if img['id'] == img_id]
    train_data = {'images' : train_data_imgs,
                 'categories' : data['categories'],
                 'annotations': [data['annotations'][idx] for idx in train_idx],}

    val_data_img_ids = set([data['annotations'][idx]['image_id'] for idx in val_idx])
    val_data_imgs = [img for img in data['images'] for img_id in val_data_img_ids if img['id'] == img_id]
    val_data = {'images' : val_data_imgs,
                'categories' : data['categories'],
               'annotations': [data['annotations'][idx] for idx in val_idx],}
    
    train_filename = os.path.join(output_dir, f'train_fold_{i}.json')
    with open(train_filename, 'w') as f:
        json.dump(train_data, f, indent=4)
        
    val_filename = os.path.join(output_dir, f'val_fold_{i}.json')
    with open(val_filename, 'w') as f:
        json.dump(val_data, f, indent=4)
```

## ë°ì´í„° íŒŒì´í”„ ë¼ì¸

<ul>
  <li><h3>ëª¨ë¸ì— random x ë”± í•˜ë‚˜ í…ŒìŠ¤íŠ¸ë¡œ ë„£ê¸°</h3></li>
</ul>

```python
# mnist ëª¨ë¸ì— ë„£ì„ 784 ì°¨ì› ë°ì´í„° 1ê°œ ì¤€ë¹„
x_numpy = np.random.rand(1,784)

# í† ì¹˜ëŠ” ëª¨ë¸ì„ ì´ìš©í•˜ë ¤ë©´ ë¬´ì¡°ê±´ í† ì¹˜ íƒ€ì…ìœ¼ë¡œ ë°”ê¿”ì•¼ í•˜ê³  cpuë“  gpuë“± ì¥ì¹˜ë¥¼ ì§€ì •í•´ì•¼ í•œë‹¤.
# to. ì•ˆí•˜ë©´ RuntimeError: Expected all tensors to be on the same device, ë°œìƒ
# í† ì¹˜ì—ì„œ ë°ì´í„° x, yì—ëŠ” deviceë¥¼ ë¬´ì¡°ê±´ ì—°ê²°í•´ì¤˜ì•¼ í•œë‹¤.
x_torch = torch.from_numpy(x_numpy).float().to(device)

y_torch = M(x_torch) # 10 ì°¨ì› ë°ì´í„° ë°˜í™˜

''' ê±°ì§“
# í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë°”ê¾¸ê¸°
# ë„˜íŒŒì´ëŠ” ë˜ cpuë¥¼ ë¨¹ì–´ì„œ 
# TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() ë°œìƒ
y_torch.numpy()

# req_gradë¥¼ ì—†ì• ê¸° ìœ„í•´ detachë¡œ ì—†ì• ë¼ê³  í•¨
# detach() methodëŠ” gradientì˜ ì „íŒŒë¥¼ ë©ˆì¶”ëŠ” ì—­í• ì„ í•œë‹¤. ì´ëŸ° ê²ƒì´ ë‹¤ ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”
# RuntimeError: Use tensor.detach().numpy() instead. ë°œìƒ
y_torch.cpu().numpy()
'''

# ì§„ì‹¤ ì½”ë“œ
y_numpy = y_torch.detach().cpu().numpy()
```

<ul>
  <li><h3>ëª¨ë¸ì— ë°ì´í„° ë°°ì¹˜ x ë”± í•˜ë‚˜ Datasetì—ì„œ êº¼ë‚´ì„œ í…ŒìŠ¤íŠ¸ë¡œ ë„£ê¸°</h3></li>
</ul>

```python
qd_train_dataset = QuickDrawDataset(train_data, train_label, transform)
x_data = next(iter(qd_train_dataset))[0].squeeze(0) # ë°°ì¹˜ 1 ë§Œë“¤ì–´ì¤˜ì•¼ í•¨
y_data = next(iter(qd_train_dataset))[1]

c = ëª¨ë¸(in_channels = 3, out_channels = 64)
y_pred = c(x_data)
y_pred
```


## ê³µí†µ

<ul>
  <li><h3>ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥</h3></li>
</ul>

```python
def save_weight_to_json(model):
  cur_dir = os.getcwd() # í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬
  ckpt_dir = "checkpoints" # weightë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
  file_name = "weights.ckpt" # ì €ì¥ íŒŒì¼ëª…
  dir = os.path.join(cur_dir, ckpt_dir) 
  os.makedirs(dir, exist_ok = True) # dirë¥¼ ë§Œë“¬

  file_path = os.path.join(dir, file_name) # dir ê²½ë¡œ + íŒŒì¼ ì´ë¦„ì˜ íŒŒì¼ ê²½ë¡œë¥¼ joiní•¨
  model.save_weights(file_path)

  model_json = model.to_json() # ëª¨ë¸ êµ¬ì¡°ë„ ì €ì¥í•˜ì—¬ model.jsonìœ¼ë¡œ ì €ì¥
  with open("model.json", "w") as json_file: 
    json_file.write(model_json)
```



# Tensorflow

<ul>
  <li><h3>ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ</h3></li>
</ul>

```python
from keras.models import model_from_json 

def load_weight_to_json():
  json_file = open("model.json", "r")
  loaded_model_json = json_file.read() 
  json_file.close()

  loaded_model = model_from_json(loaded_model_json)
  loaded_model.load_weights(file_path) # file_path = os.path.join(dir, file_name)
```


<ul>
  <li><h3>í›ˆë ¨ ì¤‘ë‹¨ í›„ ì¬ê°œì‹œ ê°€ì¤‘ì¹˜ ë¡œë“œ</h3></li>
</ul>

```python
LOAD_FROM_CK_PT = False
if LOAD_FROM_CK_PT:
    num = '00210'
    gen_model = load_model(f'output_b4_pts250/models/{num}_gen_model.h5')
    d_model = load_model(f'output_b4_pts250/models/{num}_d_model.h5')
else:
    gen_model = get_generator_model()
    d_model = get_discriminator_model()

gan_model = get_gan_model(gen_model, d_model, L1_loss_lambda=100)
```


<ul>
  <li><h3>ì†ì‹¤ ê·¸ë˜í”„ ìƒì„±</h3></li>   
</ul>

```python
def plotLoss(G_loss, D_loss, epoch):
  cur_dir = os.getcwd()
  loss_dir = "loss_graph"
  file_name = 'gan_loss_epoch_%d.png' % epoch
  dir = os.path.join(cur_dir, loss_dir) 
  os.makedirs(dir, exist_ok = True)

  file_path = os.path.join(dir, file_name)

  plt.figure(figsize=(10, 8))
  plt.plot(D_loss, label='Discriminitive loss')
  plt.plot(G_loss, label='Generative loss')
  plt.xlabel('BatchCount')
  plt.ylabel('Loss')
  plt.legend()
  plt.savefig(file_path)
```


<ul>
  <li><h3>ëª¨ë¸ ì €ì¥</h3></li>
</ul>

```python
def save_model(model, model_path='saved_model/model.h5'):
  print('\nsave model : \"{}\"'.format(model_path))
  model.save(model_path)
```


<ul>
  <li><h3>ëª¨ë¸ ë¡œë“œ</h3></li>
</ul>

```python
def load_model(model, model_path='saved_model/model.h5'):
  print('\nload model : \"{}\"'.format(model_path))
  model = tf.keras.models.load_model(model_path)
```

<ul>
  <li><h3>í›ˆë ¨ ì •í™•ë„, ì†ì‹¤ </h3></li>
</ul>

```python
import matplotlib.pyplot as plt

def show_history(history): # historyì— valë¥¼ ë½‘ìœ¼ë ¤ë©´ fití•  ë•Œ validation_dataë¥¼ ì¨ì•¼í•œë‹¤.
    plt.plot(history.history['accuracy'], label='train')
    plt.plot(history.history['val_accuracy'], label='valid')
    plt.legend()
    
def show_history(history):
    plt.plot(history.history['loss'], label='train')
    plt.plot(history.history['val_loss'], label='valid')
    plt.legend()
```

## cv -> ì°¸ê³  : ì´ë¯¸ì§€ëŠ” ë¨¸ëŸ¬(3, 4), adv(gan), ëœë“œë§ˆí¬ ì‡ê¸°

<ul>
  <li><h3>GAN ìƒì„± ì´ë¯¸ì§€ ì €ì¥</h3></li>
</ul>

```python
def sample_images(epoch, latent_dim = 128):
  cur_dir = os.getcwd()
  image_dir = "images"
  file_name = '%d.png' % epoch
  dir = os.path.join(cur_dir, image_dir) 
  os.makedirs(dir, exist_ok = True)

  file_path = os.path.join(dir, file_name)


  r, c = 5, 5
  noise = np.random.normal(0, 1, (r * c, latent_dim))
  gen_imgs = generator.predict(noise)

  # Rescale images 0 - 1
  gen_imgs = 0.5 * gen_imgs + 0.5

  fig, axs = plt.subplots(r, c)
  cnt = 0
  for i in range(r):
      for j in range(c):
          axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')
          axs[i,j].axis('off')
          cnt += 1
  fig.savefig(file_path)
  plt.close()
```

<ul>
  <li><h3>ì´ë¯¸ì§€ ë°ì´í„° ëŒì–´ëª¨ì•„ì„œ í•œ ê³³ì— ì €ì¥</h3></li>
</ul>

```python
def create_target_images():
    pathname = f'{config.ZAPPOS_DATASET_SNEAKERS_DIR}/*/*.jpg' 
    print(pathname)
    print(glob.glob(pathname)) 
  
    for filepath in glob.glob(pathname):
        filename = os.path.basename(filepath)
        img_target = load_img(filepath, target_size=(config.IMG_HEIGHT, config.IMG_WIDTH))
        img_target = np.array(img_target)  
        img_target_filepath = os.path.join(config.TRAINING_TARGET_DIR, filename) 
        save_img(img_target_filepath, img_target) 
```



<ul>
  <li><h3>í•œ í´ë”ì— ëª¨ì—¬ìˆëŠ” ì´ë¯¸ì§€ ë°ì´í„° ë‹¤ë¥¸ í´ë”ë¡œ ì˜®ê¸°ê¸°</h3></li>
</ul>


```python
def create_source_imgs(target_dir, source_dir):
    pathname = f'{target_dir}/*.jpg' # data/training/target
    print(pathname)
    for filepath in glob.glob(pathname):
        img_target = load_img(filepath, target_size=(config.IMG_HEIGHT, config.IMG_WIDTH))
        img_target = np.array(img_target)
        img_source = detect_edges(img_target)

        filename = os.path.basename(filepath)
        img_source_filepath = os.path.join(source_dir, filename)
        save_img(img_source_filepath, img_source)
```

<ul>
  <li><h3>í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ edge detection </h3></li>
</ul>

```python
import cv2
from keras_preprocessing.image import load_img, save_img

def detect_edges(img):
    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    img_gray = cv2.bilateralFilter(img_gray, 5, 50, 50)
    img_gray_edges = cv2.Canny(img_gray, 45, 100)
    img_gray_edges = cv2.bitwise_not(img_gray_edges) # invert black/white
    img_edges = cv2.cvtColor(img_gray_edges, cv2.COLOR_GRAY2RGB)
    
    return img_edges

def create_edge_imgs(target_dir, source_dir):
    pathname = f'{target_dir}/*.jpg' # target_dirì— í´ë”ëª…
    for filepath in glob.glob(pathname):
        img_target = load_img(filepath, target_size=(256, 256))
        img_target = np.array(img_target)
        img_source = detect_edges(img_target) # ì•„ ì†ŒìŠ¤ ì´ë¯¸ì§€ëŠ” ì—£ì§€ ì´ë¯¸ì§€êµ¬ë‚˜

        filename = os.path.basename(filepath)
        img_source_filepath = os.path.join(source_dir, filename)
        save_img(img_source_filepath, img_source) 
        
# ì‚¬ìš©ë²• : ì›ë³¸ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ, ì—£ì§€ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë” ê²½ë¡œ
# create_edge_imgs("/content/trainB", "/content/trainA")
```

<ul>
  <li><h3>ì´ë¯¸ì§€(jpg)ë“  ë­ë“  csvë¡œ ë§Œë“¤ê¸°</h3></li>
</ul>

```python
import os, natsort, csv, re

# file_path = 'photo/'

def toCSV(file_path):
    file_lists = os.listdir(file_path)
    file_lists = natsort.natsorted(file_lists)

    f = open('train.csv', 'w', encoding='utf-8') #valid.csv, test.csv
    wr = csv.writer(f)
    wr.writerow(["Img_name", "Class"])
    for file_name in file_lists:
        print(file_name)
        wr.writerow([file_name, re.sub('-\d*[.]\w{3}', '', file_name)])
    f.close()
```

<ul>
  <li><h3>ì´ë¯¸ì§€ ìë¥´ê¸°</h3></li>
</ul>

```python
import cv2

def seperateImg(img): # img = '/content/drive/MyDrive/Colab Notebooks/KakaoTalk_20220916_023415527.jpg'
    src = cv2.imread(img, cv2.IMREAD_UNCHANGED)
    dst1 = src[:, 0:256].copy()   # ì„ ë§Œ ìˆëŠ” ê±°
    dst2 = src[:, 257:512].copy()  # ìƒ‰ ìˆëŠ” ê±°
    cv2.imwrite('/content/drive/MyDrive/Colab Notebooks/color.jpg',dst1)  # ì €ì¥
    cv2.imwrite('/content/drive/MyDrive/Colab Notebooks/color2.jpg',dst2)  # ì €ì¥
```

<ul>
  <li><h3>í¬ë¡¤ë§ ì´ë¯¸ì§€ mnist í˜•íƒœë¡œ</h3></li>
</ul>

```python
# image_file_path = './notMNIST_small/*/*.png'

def myImage(image_file_path):
    paths = glob.glob(image_file_path)
    paths = np.random.permutation(paths)
    ë…ë¦½ = np.array([plt.imread(paths[i]) for i in range(len(paths))])
    ì¢…ì† = np.array([paths[i].split('/')[-2] for i in range(len(paths))]) # A/test.jpg -> í´ë˜ìŠ¤ A, B/test1.jpg -> í´ë˜ìŠ¤ 
    print(ë…ë¦½.shape, ì¢…ì†.shape)
```

<ul>
  <li><h3>ë„˜íŒŒì´ ì´ë¯¸ì§€ í•œ ë²ˆì— resizeí•˜ê³  jpegë¡œ ì €ì¥ê¹Œì§€</h3></li>
</ul>

```python
img_array.shape # (60000, 28, 28, 1)

def image_resize(img_array):
    for i in range(len(img_array)):
      img_resize = cv2.resize(img_array[i], dsize = (256, 256))
      img_resize = Image.fromarray(img_resize)
      img_resize = img_resize.convert('RGB')
      img_resize.save(f"{i}.jpeg")
```

<ul>
  <li><h3>train_X ì‹œê°í™”</h3></li>
</ul>

```python
import matplotlib.pyplot as plt

def visualizeTrainX():
    ncols = 10 # ì¡°ì •

    figure, axs = plt.subplots(figsize = (10, 5), nrows=1, ncols = ncols)

    for i in range(ncols):
      axs[i].imshow(train_images[i]) # (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```


## nlp -> ì°¸ê³  : í…ìŠ¤íŠ¸ëŠ” ë¨¸ëŸ¬(7, 8), ë©”íƒ€ì½”ë“œ(ì˜í™”ë¦¬ë·°), base(Q/A), ë…¼ë¬¸ ë¶„ì„ ë ˆí¬ë§Œ ë³´ë©´ ëœë‹¤,

<ul>
  <li><h3>max_len êµ¬í•˜ê¸°</h3></li>
</ul>

```python
def get_max_len(sentences):
    seq_lengths = np.array([len(s.split()) for s in sentences])
    print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])
```
